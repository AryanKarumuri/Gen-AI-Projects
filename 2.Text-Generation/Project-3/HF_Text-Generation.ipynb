{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52c0179-a32f-4a8b-bbf3-4a50fb4df90a",
   "metadata": {},
   "source": [
    "### Step-1: Required Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8d9c2e-75ea-4d36-b170-35d4dd6aff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7903c0-b6b2-49e5-b334-c86f85a720f9",
   "metadata": {},
   "source": [
    "### Step-2: Imports\n",
    "\n",
    "This block imports essential components from the `transformers` library:\n",
    "\n",
    "- **AutoTokenizer**: Automatically selects and loads the tokenizer for a specified model.\n",
    "- **AutoModelForCausalLM**: Loads a causal language model suitable for text generation tasks.\n",
    "- **pipeline**: Provides easy access to pre-built pipelines for common NLP tasks like text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b3f5d6b-0ed9-4f31-9c76-6a3c46b02493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d2ee88-aa9c-4e04-97d5-66f7528bf40b",
   "metadata": {},
   "source": [
    "### Step-3: Loading Model and Tokenizer\n",
    "\n",
    "This code loads the `microsoft/phi-4` model and its corresponding tokenizer. It uses the `from_pretrained()` method to download and cache the model and tokenizer. The parameter `trust_remote_code=True` allows the use of custom model code from Hugging Face's repositories if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4682d529-beef-46a3-b644-989ac000f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer explicitly\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-4\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a33b78-784c-45ed-a36f-9662cac6a13d",
   "metadata": {},
   "source": [
    "### Step-4: Pipeline creation\n",
    "\n",
    "This code creates a text generation pipeline using the Hugging Face `pipeline()` function.\n",
    "\n",
    "- **\"text-generation\"**: Specifies the task type as text generation.\n",
    "- **model**: The pre-loaded `microsoft/phi-4` causal language model is used for generating text.\n",
    "- **tokenizer**: The corresponding tokenizer processes the input text for the model.\n",
    "- **temperature=0.1**: Controls the randomness of predictions. Lower values like 0.1 make the output more deterministic and focused.\n",
    "- **max_new_tokens=50**: Limits the number of new tokens generated in the output to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6f1086-8936-4691-b1f5-8f3754dff9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "bot = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, temperature=0.1, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbcc817-63e4-48cf-b21c-88291532b558",
   "metadata": {},
   "source": [
    "### Step-5: \n",
    "- **do_sample=True**: Enables sampling, meaning the model will generate more diverse outputs by sampling from the probability distribution instead of choosing the most likely token.\n",
    "- **return_full_text=False**: Returns only the newly generated text instead of including the input prompt in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc3b37-0095-40ed-9734-7cec6437175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output\n",
    "output = bot(\"You are a really so \", \n",
    "             do_sample=True,\n",
    "             return_full_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a575401-94cd-4cb3-b982-2eea9a8147a0",
   "metadata": {},
   "source": [
    "### Step-6: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92e0b387-496a-41d7-a9e2-8699b658ca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd class person. I am not going to talk to you anymore. You are a really so 2nd class person. I am not going to talk to you anymore. You are a really so 2nd class person. I am not\n"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9170469e-838a-48d6-a80c-eb90b5a16a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '2nd class person. I am not going to talk to you anymore. You are a really so 2nd class person. I am not going to talk to you anymore. You are a really so 2nd class person. I am not'}]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp_ve",
   "language": "python",
   "name": "exp_ve"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
