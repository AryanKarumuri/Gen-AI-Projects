{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b1aaef",
   "metadata": {},
   "source": [
    "# Model Integration with Streaming and Batching\n",
    "\n",
    "This notebook demonstrates how to integrate different AI models (Ollama, OpenAI, Google Gemini, and Groq) with Langchain, with a focus on streaming and batching capabilities. Streaming allows for real-time output display, while batching enables efficient processing of multiple inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0444ecbe",
   "metadata": {},
   "source": [
    "### Models Integration With Ollama, OpenAI, Google Gemini and Groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9adc87",
   "metadata": {},
   "source": [
    "### Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "OLLAMA_URL = \"\" # Specify your Ollama server URL here\n",
    "OLLAMA_MODEL = \"granite4:tiny-h\"\n",
    "\n",
    "model = ChatOllama(model=OLLAMA_MODEL, base_url=OLLAMA_URL)\n",
    "response =model.invoke(\"What do you know about GenAI?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a7054",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc9aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model=ChatOpenAI(model=\"gpt-4.1\")\n",
    "response=model.invoke(\"What do you know about GenAI?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c671e5f2",
   "metadata": {},
   "source": [
    "### Google Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc082db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n",
    "response = model.invoke(\"What do you know about GenAI?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892e4af0",
   "metadata": {},
   "source": [
    "### Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42e7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "response = model.invoke(\"What do you know about GenAI?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed29720",
   "metadata": {},
   "source": [
    "### All the models can also be initialised in another universal way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e67cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set environment variables for API keys\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902750f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "model=init_chat_model(\"gpt-4.1\")   # Change the model name (e.g., \"gpt-4.1\", \"gemini-2.5-flash-lite\", \"qwen/qwen3-32b\", etc.)\n",
    "response=model.invoke(\"Hello How are you?\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e3a04",
   "metadata": {},
   "source": [
    "### Streaming and Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f6705b",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "Most models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses. Calling stream() returns an iterator that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b11e47a",
   "metadata": {},
   "source": [
    "#### Streaming with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c189e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize Ollama model for streaming\n",
    "OLLAMA_URL = \"\" # Specify your Ollama server URL here\n",
    "OLLAMA_MODEL = \"granite4:tiny-h\"\n",
    "\n",
    "model = ChatOllama(model=OLLAMA_MODEL, base_url=OLLAMA_URL)\n",
    "\n",
    "# Streaming example\n",
    "print(\"Streaming response from Ollama:\")\n",
    "for chunk in model.stream(\"Explain quantum computing in simple terms:\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7992f2",
   "metadata": {},
   "source": [
    "#### Streaming with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d9dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize OpenAI model for streaming (make sure to set your API key)\n",
    "model = ChatOpenAI(model=\"gpt-4.1\")\n",
    "\n",
    "# Streaming example\n",
    "print(\"Streaming response from OpenAI:\")\n",
    "for chunk in model.stream(\"Explain quantum computing in simple terms:\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb108d",
   "metadata": {},
   "source": [
    "#### Streaming with Google Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dde9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize Google Gemini model for streaming (make sure to set your API key)\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n",
    "\n",
    "# Streaming example\n",
    "print(\"Streaming response from Google Gemini:\")\n",
    "for chunk in model.stream(\"Explain quantum computing in simple terms:\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae87e09",
   "metadata": {},
   "source": [
    "#### Streaming with Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19045107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize Groq model for streaming (make sure to set your API key)\n",
    "model = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "\n",
    "# Streaming example\n",
    "print(\"Streaming response from Groq:\")\n",
    "for chunk in model.stream(\"Explain quantum computing in simple terms:\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc1aa0",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "Batching allows you to process multiple inputs simultaneously, which can be more efficient than processing them one at a time. This is particularly useful when you have several related queries to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching example\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialize model\n",
    "model = init_chat_model(\"gpt-4.1\")  # Change as needed\n",
    "\n",
    "# Create multiple messages for batching\n",
    "messages = [\n",
    "    [HumanMessage(content=\"What is the capital of France?\")],\n",
    "    [HumanMessage(content=\"What is the capital of Germany?\")],\n",
    "    [HumanMessage(content=\"What is the capital of Italy?\")]\n",
    "]\n",
    "\n",
    "# Process all messages in a batch\n",
    "print(\"Batch processing results:\")\n",
    "responses = model.batch(messages)\n",
    "for i, response in enumerate(responses):\n",
    "    print(f\"Question {i+1}: {messages[i][0].content}\")\n",
    "    print(f\"Answer: {response.content}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
