{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c11ce1d-931b-4118-8986-36223c20b058",
   "metadata": {},
   "source": [
    "## RAG using MergerRetriever and LongContextReorder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ddc228-9eb4-4fb1-a112-184c1ab43b38",
   "metadata": {},
   "source": [
    "**Introduction to RAG using MergerRetriever and LongContextReorder**\n",
    "\n",
    "\"RAG using MergerRetriever and LongContextReorder\" enhances the Retrieval-Augmented Generation (RAG) framework by combining a hybrid retrieval system (MergerRetriever) and long context management (LongContextReorder). MergerRetriever blends sparse and dense retrieval methods to gather diverse, relevant documents, while LongContextReorder optimizes and prioritizes important sections of long documents. This approach improves the accuracy and coherence of model-generated responses by ensuring the most relevant information is used effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750c816-e6e1-4411-86a7-30e5588ee4b0",
   "metadata": {},
   "source": [
    "## Key Differences Between \"RAG using MergerRetriever and LongContextReorder\" and \"RAG using Hybrid Retrievers and Reranking\"\n",
    "\n",
    "### 1. Focus:\n",
    "- **MergerRetriever and LongContextReorder**: Focuses on `handling and optimizing long documents` and ensuring `diverse retrieval`.\n",
    "- **Hybrid Retrievers and Reranking**: Focuses on improving `retrieval accuracy` and `document relevance` through `reranking`.\n",
    "\n",
    "### 2. Document Management:\n",
    "- **MergerRetriever and LongContextReorder**: Uses `LongContextReorder` to prioritize and optimize `long context documents`, ensuring the most relevant sections are passed to the model.\n",
    "- **Hybrid Retrievers and Reranking**: Uses `reranking` to refine the choice of documents after retrieval, enhancing the quality of the final input based on relevance.\n",
    "\n",
    "### 3. Retrieval Process:\n",
    "- **MergerRetriever and LongContextReorder**: Combines multiple retrieval methods and optimizes long context data for more effective document handling.\n",
    "- **Hybrid Retrievers and Reranking**: Combines multiple retrieval methods and uses an additional `reranking step` to prioritize the most relevant documents.\n",
    "\n",
    "### 4. Objective:\n",
    "- **MergerRetriever and LongContextReorder**: Primarily aims to manage `long and diverse contexts` for improved output quality.\n",
    "- **Hybrid Retrievers and Reranking**: Primarily aims to refine the retrieval process and prioritize `relevant documents` for the best possible generation output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff684c2e-c569-4239-b407-2ac3cf8f9b52",
   "metadata": {},
   "source": [
    "### Step-1: Required Package Installation\n",
    "\n",
    "These dependencies will set up a complete environment for working on a RAG system using Flash Reranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d8a0bf-257c-4c9f-8e68-c78158d809d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain langchain_community huggingface-hub langchain-huggingface langchain-text-splitters chromadb langchain_groq pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8513b53-81e1-408e-9ae3-6513789e75c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers==2.2.2 InstructorEmbedding==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d991c576-5352-4312-88a6-afb6ba5737b3",
   "metadata": {},
   "source": [
    "### Step-2: Imports\n",
    "\n",
    "These imports set up an environment for out tryout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6fdca-b2a7-4736-acb3-6fea07abd8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pypdf\n",
    "import langchain\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.document_transformers import EmbeddingsClusteringFilter, EmbeddingsRedundantFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f921f192-d9b9-431b-8ff6-2c4e8a6fa12f",
   "metadata": {},
   "source": [
    "### Step-3: LLM Setup\n",
    "\n",
    "In this step, we will be using the `llama-3.1-8b-instant` model from GROQ. To access and use the model, you will need to create an API key. \n",
    "Need steps for generate your API key, visit the following link: [GROQ API_Key_Generation](https://github.com/AryanKarumuri/Gen-AI-Projects/blob/main/README.md#api-key-generation-guide) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8917e075-2cdf-43df-8837-020c72717c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if GROQ_API_KEY:\n",
    "    llm=ChatGroq(groq_api_key=GROQ_API_KEY,model_name=\"llama-3.1-8b-instant\")\n",
    "    print(GROQ_API_KEY)\n",
    "else:\n",
    "    print(\"Add Groq API Key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40340d3-919a-4094-bc14-9a089427bd76",
   "metadata": {},
   "source": [
    "### Step-4: Documents Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f109f2-2fa0-4e0f-a672-68f68c85c9ff",
   "metadata": {},
   "source": [
    "#### First document loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09ac8a33-a784-45d5-8de6-dc23dcf544d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_loader = PyPDFLoader(\"./data/Orca_paper.pdf\")\n",
    "orca_document = orca_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "465f233d-4ca1-4eef-9b14-009cb3140617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "print(len(orca_document))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb830505-951d-4d79-a796-27c6559a0ab5",
   "metadata": {},
   "source": [
    "#### Second document loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff2567b8-db91-4b17-a6aa-8c060c67a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssra_loader = PyPDFLoader(\"./data/semantic_search_&_recommendation_algorithms.pdf\")\n",
    "ssra_document = ssra_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bf0a060-509f-431d-8a67-d4d128725b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(ssra_document))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7187792-2d15-471b-a9f4-b50c54cb15d0",
   "metadata": {},
   "source": [
    "### Step-5: Documents Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a8e3f18-0379-4d29-bdb4-44d65a212074",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639f6c0-b7b4-41d0-bdea-11fa3ef67ba1",
   "metadata": {},
   "source": [
    "#### First document splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "617c2c34-6ad1-473d-855d-ea0350a43d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382\n"
     ]
    }
   ],
   "source": [
    "text_orca = text_splitter.split_documents(orca_document)\n",
    "print(len(text_orca))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58affc91-2ed4-4faa-8de8-633fb6186de1",
   "metadata": {},
   "source": [
    "#### Second document splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67e76dd6-30ad-49c9-82f7-ed939b261c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "text_ssra = text_splitter.split_documents(ssra_document)\n",
    "print(len(text_ssra))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf9624-ca1f-4d4b-b354-9f15e014a4b2",
   "metadata": {},
   "source": [
    "### Step-6: Loading Embedding Model\n",
    "\n",
    "- **`hf_embeddings`** is an instance of `HuggingFaceEmbeddings` using the `sentence-transformers/all-MiniLM-L6-v2` model, which generates dense vector representations for sentences. This model is efficient, offering high-quality embeddings for tasks like semantic similarity and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7015d83-207d-4926-b868-b8280d61500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeddings\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ae912-a25c-4dfc-a9ef-6deccb9adb55",
   "metadata": {},
   "source": [
    "### Step-7: Database Creation and DB Retriever\n",
    "\n",
    "- **Database Creation** (`Chroma.from_documents()`): Creates a vector store using **Chroma**, which indexes the document chunks and stores their embeddings in a collection.\n",
    "- **DB Retriever** (`vector_store.as_retriever()`): Converts the vector store into a retriever, allowing for efficient querying and retrieval of relevant documents based on vector similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fae36a-23a9-44ee-bcc0-b2bd2f2027ad",
   "metadata": {},
   "source": [
    "#### db_collection-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ce816ba-e370-4ef2-9f1e-34d202e11f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_vector_store = Chroma.from_documents(\n",
    "    documents=text_orca,       \n",
    "    embedding=hf_embeddings,    \n",
    "    collection_name=\"db_collection-1\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f897f1-38f8-4445-b658-626c3a4f8ff0",
   "metadata": {},
   "source": [
    "#### retriever-1 for db_collection-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bbe491cc-c12e-41a3-9e84-526cba83925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_orca = orca_vector_store.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d782c7-d81e-4704-82c9-51b68dd53203",
   "metadata": {},
   "source": [
    "#### db_collection-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3092ecf-675e-4e18-9d8e-53ae704055e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssra_vector_store = Chroma.from_documents(\n",
    "    documents=text_ssra,       \n",
    "    embedding=hf_embeddings,    \n",
    "    collection_name=\"db_collection-2\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37a6402-3c3b-415c-8def-81befc3a1474",
   "metadata": {},
   "source": [
    "#### retriever-2 for db_collection-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "489120ad-f562-4dd1-b67d-c8f9af0b38b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_ssra = ssra_vector_store.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca69c440-95a5-44c5-a3cd-6c6e61fe194f",
   "metadata": {},
   "source": [
    "### Step-8: [LOTR(Lord Of Retriever)](https://python.langchain.com/docs/integrations/retrievers/merger_retriever/)\n",
    "\n",
    "Lord of the Retrievers (LOTR), or MergerRetriever, combines the results of multiple retrievers'. It enhances retrieval accuracy by reducing bias and prioritizing the most relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b52a2a0-21a1-4762-905c-36bfe511f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr = MergerRetriever(retrievers=[retriever_orca, retriever_ssra])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c86fc-ba0f-4371-8f04-b0052b7dd754",
   "metadata": {},
   "source": [
    "#### Let's ask question from `orca.pdf` and check whether it is able retrieve or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "32e6b50b-f5ef-4b58-94c5-475a5391190e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 4: Instruction-tuning with GPT-49. Given user instructions for a task and an input,\n",
      "the system generates a response. Existing works like Alpaca [7], Vicuna [9] and variants\n",
      "follow a similar template to train small models with⟨{user instruction, input}, output⟩.\n",
      "2 Preliminaries\n",
      "2.1 Instruction Tuning\n",
      "Instruction tuning [22] is a technique that allows pre-trained language models to learn\n",
      "from input (natural language descriptions of the task) and response pairs, for example,\n",
      "latency of the search process, enabling real-time data retrieval\n",
      "even with large-scale datasets.\n",
      "E. Evaluation and Testing\n",
      "To assess the performance of the system, several key metrics\n",
      "are used to evaluate both the accuracy of the results and the\n",
      "efficiency of the retrieval process. These metrics include:\n",
      "Precision: Measures the accuracy of relevant results re-\n",
      "trieved. Recall: Evaluates the completeness of the retrieved\n",
      "results. F1-Score: Combines precision and recall to provide a\n",
      "Contents\n",
      "1 Introduction 4\n",
      "1.1 Challenges with Existing Methods . . . . . . . . . . . . . . . . . . . . . . . . 5\n",
      "1.2 Key Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n",
      "2 Preliminaries 7\n",
      "2.1 Instruction Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
      "2.2 Role of System Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
      "3 Explanation Tuning 8\n",
      "The proposed method outperformed traditional algorithms\n",
      "consistently.\n",
      "Fig. 2. Comparison of search accuracy between the proposed method and\n",
      "traditional search methods across various dataset sizes.\n",
      "Figure 2 highlights the superior performance of our ap-\n",
      "proach, especially in larger datasets where traditional methods\n",
      "falter due to their inability to effectively parse and understand\n",
      "semantic nuances.\n",
      "B. Efficiency in Response Times\n",
      "To evaluate the efficiency of the Annoy Index, response\n",
      "from input (natural language descriptions of the task) and response pairs, for example,\n",
      "{\"instruction\": \"Arrange the words in the given sentence to form a grammatically\n",
      "correct sentence.\", \"input\": \"the quickly brown fox jumped\", \"output\": \"the brown\n",
      "fox jumped quickly\"}. Instruction tuning has been applied to both language-only and\n",
      "multimodal tasks. For language-only tasks, instruction tuning has been shown to improve\n",
      "A. Data Collection and Preparation\n",
      "Effective data preparation is a crucial step in any data-driven\n",
      "project, especially in large-scale systems. For this project, we\n",
      "used a large dataset from Kaggle NEET 2024 dataset. con-\n",
      "taining over 2 million rows of student data, including scores,\n",
      "center information, location details, and student identifiers.\n",
      "The goal was to preprocess this data to ensure its quality and\n",
      "consistency for semantic processing.\n",
      "import pandas as pd\n",
      "Load and preprocess the data\n",
      "Model Tuning Method Data Size Teacher\n",
      "Alpaca Simple Instructions / Self-instruct 52K text-da-vinci-003\n",
      "Vicuna User Instructions / Natural 70K ChatGPT\n",
      "Dolly User Instructions / Natural 15K Human\n",
      "WizardLM Complex Instructions / Evol-instruct 250K ChatGPT\n",
      "Orca Complex Instructions / Explanations 5M ChatGPT (5M)\n",
      "∩GPT-4 (1M)\n",
      "Table 1: Overview of popular models instruction tuned with OpenAI large foundation models\n",
      "(LFMs). Orca leverages complex instructions and explanations for progressive learning.\n",
      "inherent complexity and ambiguity of natural language. These\n",
      "systems lack the ability to understand the semantic meaning\n",
      "and context of queries, leading to inaccurate results and\n",
      "suboptimal user experiences. The challenges of traditional\n",
      "search methods become particularly evident when dealing with\n",
      "high-dimensional data or complex, multifaceted queries, which\n",
      "require a deeper level of understanding.\n",
      "The evolution of semantic search technologies aims to\n",
      "of the size of data and tuning methods employed in recent popular instruction tuning works.\n",
      "Limited imitation signals. Existing methods rely on immitation learning from\n",
      "⟨query, response⟩pairs generated by the teacher model. However, this provides limited\n",
      "signals to trace the reasoning process of the teacher. Prior works [15, 16] on open-box model\n",
      "show that richer signals such as logits, intermediate representations and attention states can\n",
      "Workshop on Data Engineering for Big Data, ACM Press, 2015, ISBN:\n",
      "978-1450331712.\n",
      "[8] ANNOY documentation by Zilliz: https://zilliz.com/learn/approximate-\n",
      "nearest-neighbor-oh-yeah-ANNOY\n",
      "[9] Bernhardsson, E. (2024). Annoy at GitHub: Approximate Nearest Neigh-\n",
      "bors in C++/Python Optimized for Memory Usage and Loading/Saving\n",
      "to Disk. Retrieved from https://github.com/spotify/annoy.\n",
      "[10] ANNOY benchmarks: https://ann-benchmarks.com/annoy.html\n",
      "[11] Contextual String Embeddings for Sequence Labeling:\n"
     ]
    }
   ],
   "source": [
    "for chunks in lotr.invoke(\"what is Instruction Tuning?\"):\n",
    "    print(chunks.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5239706b-8f1d-4d65-a87b-be17cd2fb8ce",
   "metadata": {},
   "source": [
    "#### Let's ask question from `semantic_search_&_recommendation_algorithms.pdf` and check whether it is able retrieve or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49650660-6311-4873-b9a6-39976b0b7d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Biases:Large language models, trained on extensive data, can inadvertently carry\n",
      "biases present in the source data. Consequently, the models may generate outputs that could\n",
      "be potentially biased or unfair.\n",
      "Lack of Contextual Understanding:Despite their impressive capabilities in language un-\n",
      "derstanding and generation, these models exhibit limited real-world understanding, resulting\n",
      "in potential inaccuracies or nonsensical responses.\n",
      "vector representations.\n",
      "B. Word2Vec Model for Semantic Understanding\n",
      "Word2Vec is used to convert textual data into vector em-\n",
      "beddings, capturing the semantic relationships between words\n",
      "based on their usage in context. By training the model on a\n",
      "large corpus of data, Word2Vec allows the semantic meaning\n",
      "of terms to be represented in a dense vector space, where\n",
      "similar words are clustered closer together.\n",
      "from gensim.models import Word2Vec from\n",
      "nltk.tokenize import word_tokenize\n",
      "[33] Tommaso Caselli, Valerio Basile, Jelena Mitrovic, and M. Granitzer. Hatebert: Retraining bert\n",
      "for abusive language detection in english.ArXiv, abs/2010.12472, 2021.\n",
      "[34] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan\n",
      "Cao. React: Synergizing reasoning and acting in language models. InInternational Conference\n",
      "on Learning Representations, 2023.\n",
      "[35] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-\n",
      "from gensim.models import Word2Vec from\n",
      "nltk.tokenize import word_tokenize\n",
      "Tokenize text and train Word2Vec model\n",
      "sentences = [word_tokenize(sent) for sent\n",
      "in df[’state’].tolist()]\n",
      "model = Word2Vec(sentences,\n",
      "vector_size=100, window=5, min_count=1,\n",
      "workers=4) model.save(\"word2vec.model\")\n",
      "The Word2Vec parameters (such as vector size, window\n",
      "size, and minimum word count) were chosen based on pre-\n",
      "liminary experiments to balance performance and semantic\n",
      "Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\n",
      "language models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "[11] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won\n",
      "Chung, AakankshaChowdhery, QuocVLe, EdHChi, DennyZhou, , andJasonWei. Challenging\n",
      "big-bench tasks and whether chain-of-thought can solve them.arXiv preprint arXiv:2210.09261,\n",
      "2022.\n",
      "experience and increased sales.\n",
      "C. Potential Limitations and Future Research Directions\n",
      "While the results are promising, the study is not without\n",
      "limitations. The dependency on high-quality, pre-processed\n",
      "datasets for training the Word2Vec model is a significant one,\n",
      "as the quality of vector representations directly influences the\n",
      "search accuracy. Future research could explore ways to en-\n",
      "hance the robustness of the model training phase, possibly by\n",
      "in Table 10. We observe that scaling the amount of explanation data by5×with intermediate\n",
      "ChatGPT assistance significantly improves the model performance by4.5 points on aggregate.\n",
      "18\n",
      "mance remain significant challenges. Our project, which inte-\n",
      "grates Word2Vec for deep semantic understanding and Annoy\n",
      "Index for efficient nearest neighbor search, addresses these\n",
      "[24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\n",
      "[25] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\n",
      "Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al.\n",
      "Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In\n",
      "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,\n",
      "pages 5085–5109, 2022.\n",
      "consistency for semantic processing.\n",
      "import pandas as pd\n",
      "Load and preprocess the data\n",
      "df = pd.read_csv(’NEET_2024_RESULTS.csv’)\n",
      "df.dropna(inplace=True)\n",
      "# Remove any missing data\n",
      "The data cleaning process involved removing rows with\n",
      "missing values, ensuring the dataset was complete and ready\n",
      "for semantic processing. Additionally, we focused on the\n",
      "textual information (e.g., center names, state names) to build\n",
      "vector representations.\n",
      "B. Word2Vec Model for Semantic Understanding\n"
     ]
    }
   ],
   "source": [
    "for chunks in lotr.invoke(\"Explain about Word2Vec Model for Semantic Understanding?\"):\n",
    "    print(chunks.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b6eede-46f3-47d8-be2d-c9d1e61c81e5",
   "metadata": {},
   "source": [
    "### Step-9: Retriever Setup\n",
    "\n",
    "1. **Embedding Filter:**\n",
    "   The `EmbeddingsRedundantFilter` is used to filter out redundant embeddings from a set of embeddings. This step helps in eliminating unnecessary or duplicate information, ensuring that the embeddings contain only the most relevant features for downstream processing.\n",
    "\n",
    "2. **Reordering:**\n",
    "   The `LongContextReorder` component is responsible for reordering the content within a document. This process considers the long-term context of the document, ensuring that the information is structured in a more optimal way for better comprehension and usage.\n",
    "\n",
    "3. **Document Compression Pipeline:**\n",
    "   A `DocumentCompressorPipeline` is created by chaining together the `embedding_filter` and `reordering` steps. This pipeline compresses documents by first filtering out redundant embeddings and then reordering the content to preserve the most critical information while reducing unnecessary parts.\n",
    "\n",
    "4. **Contextual Compression Retriever:**\n",
    "   The `ContextualCompressionRetriever` utilizes the previously defined compression pipeline and combines it with a base retriever (`lotr`). It is designed to retrieve the most relevant documents after they have been compressed. The retrieval process is customized with search parameters, specifying that the top 3 results should be returned based on the compressed context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4fd9dd7b-5b66-4167-aded-c1e91c1b53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_filter = EmbeddingsRedundantFilter(embeddings=hf_embeddings)\n",
    "reordering = LongContextReorder()\n",
    "pipeline = DocumentCompressorPipeline(transformers=[embedding_filter, reordering])\n",
    "compression_retriever_reordered = ContextualCompressionRetriever(\n",
    "            base_compressor=pipeline, base_retriever=lotr,search_kwargs={\"k\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6311f5-9c46-477e-b9d4-28da478b226b",
   "metadata": {},
   "source": [
    "### Step-10: Chain Setup\n",
    "The code uses a **RetrievalQA** chain to perform a question-answering task using a compressed retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3bc071ed-fcc4-4cf4-9183-5a05b72fc03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "      llm=llm,\n",
    "      chain_type=\"stuff\",\n",
    "      retriever = compression_retriever_reordered,\n",
    "      return_source_documents = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "caa39384-49e9-47c0-9f81-5593e4ba3a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4032033/1342850755.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = qa(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the given context, the Comparison of Search Accuracy refers to the assessment of the proposed search method against traditional search algorithms. The search accuracy was evaluated by measuring precision, recall, and F1-scores across datasets of increasing complexity.\n",
      "\n",
      "Here's a breakdown of the key points related to the Comparison of Search Accuracy:\n",
      "\n",
      "1. **Assessment Metrics**: The search accuracy was assessed using precision, recall, and F1-scores. Precision measures the accuracy of relevant results retrieved, recall evaluates the completeness of the retrieved results, and F1-score combines precision and recall to provide a comprehensive measure of accuracy.\n",
      "2. **Dataset Complexity**: The assessment was conducted across datasets of increasing complexity, indicating that the proposed method was tested on a range of data scenarios.\n",
      "3. **Comparison with Traditional Algorithms**: The proposed method outperformed traditional algorithms in terms of search accuracy, precision, recall, and F1-scores. This suggests that the new search method is more effective in retrieving relevant information from complex datasets.\n",
      "4. **Advantages of the Proposed Method**: The results demonstrate clear advantages of the proposed method over traditional search methods, particularly in terms of accuracy, response times, and resource utilization.\n",
      "\n",
      "The Comparison of Search Accuracy is an essential aspect of evaluating the performance of the proposed search method, as it provides a quantitative measure of the method's effectiveness in retrieving relevant information from complex datasets.\n"
     ]
    }
   ],
   "source": [
    "query =\"Explain about Comparison of Search Accuracy?\"\n",
    "results = qa(query)\n",
    "print(results['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09029b6a-7137-4736-a5c8-b3a15b506a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[\"source_documents\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytryout_venv",
   "language": "python",
   "name": "mytryout_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
