{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b9d8af-0217-4653-a26b-968204e5b094",
   "metadata": {},
   "source": [
    "## Vanilla RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "Vanilla RAG (Retrieval-Augmented Generation) is a model architecture that combines retrieval-based and generative approaches to improve natural language understanding and generation tasks. In vanilla RAG, the model retrieves relevant information from an external knowledge base (such as a document store or search index) and then uses that retrieved information to generate more accurate and contextually rich responses. This setup enhances the model's ability to handle open-domain questions or tasks where a large external knowledge source is necessary.\n",
    "\n",
    "Vanilla RAG typically consists of two key components:\n",
    "\n",
    "1. **Retriever**: Retrieves relevant documents or passages based on the input query.\n",
    "2. **Generator**: Uses the retrieved documents to generate a response, often leveraging a transformer-based language model.\n",
    "\n",
    "This hybrid approach helps overcome the limitations of solely generative or retrieval models, making it ideal for tasks that require both precise information retrieval and coherent text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec535802-99f2-4313-ab8e-5d284e7ff9f2",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook helps to create a Retrieval-Augmented Generation (RAG) model using **Langchain**. It walks through the entire pipeline from loading data to creating a custom RAG chain for generating responses.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Required Package Installation\n",
    "2. Imports\n",
    "3. Loading and Splitting\n",
    "4. Post-Processing\n",
    "5. Getting Embeddings\n",
    "6. Database Creation\n",
    "7. LLM by Groq\n",
    "8. System Prompt\n",
    "9. RAG Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de93f0-bc98-416a-aa3c-e4c03c0afa09",
   "metadata": {},
   "source": [
    "### Step-1: Required Package Installation\n",
    "\n",
    "These dependencies will set up a complete environment for working on a RAG system using Langchain, along with embeddings, document retrieval, and generative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debebf97-e89c-49d8-8d5c-8400b905e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.44.2 langchain==0.3.3 \\\n",
    "                             langchain-community==0.3.0 \\\n",
    "                             langchain-core==0.3.10 \\\n",
    "                             langchain-text-splitters==0.3.0 \\\n",
    "                            chroma-hnswlib==0.7.6 \\\n",
    "                             chromadb==0.5.11 \\\n",
    "                             accelerate==1.0.1 \\\n",
    "                             pypdf \\\n",
    "                             ipywidgets \\\n",
    "                            langchain-groq \\\n",
    "                            huggingface-hub==0.25.1 \\\n",
    "                            langchain-huggingface==0.1.0 \\\n",
    "                            InstructorEmbedding==1.0.1 \\\n",
    "                             rank-bm25==0.2.2\n",
    "!pip install sentence-transformers==2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243f8f70-c0d8-474a-bf22-0fce10ad2898",
   "metadata": {},
   "source": [
    "### Step-2: Imports\n",
    "\n",
    "These imports set up an environment that integrates document loading, embeddings, vector stores, and interactions with large language models (LLMs), making it suitable for building RAG (Retrieval-Augmented Generation) systems and other NLP workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b45083a0-57ad-4f98-94ed-c2c522cb9f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Chroma and related imports\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Langchain related imports\n",
    "import langchain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb4e46-ef9e-4281-810f-f35e6fa1acff",
   "metadata": {},
   "source": [
    "### Step-3: Loading & Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0520223-d192-4e85-ad6c-c0a9e0b923b7",
   "metadata": {},
   "source": [
    "1. **PDF Loading**: Loads the content of a PDF file located at `pdf_path`.\n",
    "\n",
    "2. **Document Splitting**: Splits the loaded PDF text into smaller chunks of 1000 characters, with an overlap of 300 characters between chunks. This helps in managing large documents by breaking them into smaller, more manageable parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a04508-0865-4901-ab88-f8269b55db28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Orca: Progressive Learning from Complex\n",
      "Explanation Traces of GPT-4\n",
      "Subhabrata Mukherjee∗†, Arindam Mitra∗\n",
      "Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah\n",
      "Microsoft Research\n",
      "Abstract\n",
      "Recent research has focused on enhancing the capability of smaller models\n",
      "through imitation learning, drawing on the outputs generated by large\n",
      "foundation models (LFMs). A number of issues impact the quality of these\n",
      "models, ranging from limited imitation signals from shallow LFM outputs;\n",
      "small scale homogeneous training data; and most notably a lack of rigorous\n",
      "evaluation resulting in overestimating the small model’s capability as they\n",
      "tend to learn to imitate the style, but not the reasoning process of LFMs . To\n",
      "address these challenges, we develop Orca, a 13-billion parameter model\n",
      "that learns to imitate the reasoning process of LFMs. Orca learns from\n",
      "rich signals from GPT-4 including explanation traces; step-by-step thought' metadata={'source': 'data/Orca_paper.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "# PDF Loading\n",
    "pdf_path = \"./Orca_paper.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "docs = loader.load()\n",
    "if not docs:\n",
    "    raise ValueError(\"No documents loaded from the PDF.\")\n",
    "\n",
    "# Splitting docs into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=300)\n",
    "document_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(document_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b7458-0186-42ff-934f-a1b8849548e6",
   "metadata": {},
   "source": [
    "### Step-4: Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7115b192-240d-4154-b7f7-7dd717671059",
   "metadata": {},
   "source": [
    "1. **`format_docs(docs)`**: The function takes a list of documents (`docs`) and joins their `page_content` into a single string. The content is separated by two newline characters (`\\n\\n`) for better readability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf7d6be8-1eeb-49c6-a9b8-f5fb3dd08b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df139c6-20f5-4ff2-b868-4b0f312a01c2",
   "metadata": {},
   "source": [
    "### Step-5: Getting Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7395b7b-0c84-4850-ac9d-9a35c5f93eca",
   "metadata": {},
   "source": [
    "1. **`EMBEDDING_MODEL_NAME`**: Specifies the name of the pre-trained embedding model, which is `\"hkunlp/instructor-large\"`. This model is designed for generating embeddings.\n",
    "\n",
    "2. **`get_embeddings()`**: Initializes the `HuggingFaceInstructEmbeddings` with the given model name and provides instructions for how to represent documents and queries for retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30740a34-018c-4797-adc7-941f53cc8019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained Embedding Model.\n",
    "EMBEDDING_MODEL_NAME = \"hkunlp/instructor-large\"\n",
    "\n",
    "#Embeddings\n",
    "def get_embeddings():\n",
    "    embeddings = HuggingFaceInstructEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL_NAME,\n",
    "            embed_instruction=\"Represent the document for retrieval:\",\n",
    "            query_instruction=\"Represent the question for retrieving supporting documents:\"\n",
    "        )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d697c82-4cf4-416a-9e0a-92d667e5a4b7",
   "metadata": {},
   "source": [
    "### Step-6: Database Creation and DB Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2731d6-3225-4b18-93a7-eb0fd975f751",
   "metadata": {},
   "source": [
    "1. **Database Creation** (`Chroma.from_documents()`): Creates a vector store using **Chroma**, which indexes the document chunks and stores their embeddings in a collection.\n",
    "   \n",
    "2. **DB Retriever** (`vector_store.as_retriever()`): Converts the vector store into a retriever, allowing for efficient querying and retrieval of relevant documents based on vector similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc07e808-ade9-4480-9509-e2dcf9ee603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Creation\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=document_chunks,       \n",
    "    embedding=get_embeddings(),    \n",
    "    collection_name=\"db_collection\"  \n",
    ")\n",
    "\n",
    "# DB Retriever\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcfaf4c-2e2b-4286-92ef-629ef2c6d4c8",
   "metadata": {},
   "source": [
    "### Step-7: LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ca43f88-e1e4-4736-96c0-139359be515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM \n",
    "api_key = \"<Your_API_Key>\"\n",
    "if api_key:\n",
    "    llm=ChatGroq(groq_api_key=api_key,model_name=\"Gemma2-9b-It\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011dc82b-09f5-40a8-8c53-b0d64ed2d249",
   "metadata": {},
   "source": [
    "### Step-8: System Prompt\n",
    "\n",
    "1. **System Prompt**: Defines the behavior of the assistant for question-answering tasks. It ensures that the assistant answers strictly based on the provided context and informs the user when the answer is unknown.\n",
    "\n",
    "2. **Prompt Template** (`ChatPromptTemplate.from_template()`): Creates a flexible prompt template using the system prompt. It replaces the placeholders `{context}` and `{question}` with actual values at runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d9cd999-9726-40f8-8e58-610868a7c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer the question. \"\n",
    "        \"You must answer questions strictly using the provided context. If you don't know the answer, say that you don't know. \"\n",
    "        \"\\n\\n\"\n",
    "        \"{context}\"\n",
    "        \"Question: {question}\"\n",
    "    )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8ae1a-1b8c-4274-86d2-b54313b1d4e0",
   "metadata": {},
   "source": [
    "### Step-9: RAG Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6934af-c6b6-4ff0-83f9-bfcb709bad4e",
   "metadata": {},
   "source": [
    "1. **Context and Question Preparation**: Retrieves the context using the retriever, formats it, and passes the question as-is without modification.\n",
    "\n",
    "2. **Prompt Application**: Applies the system prompt, which integrates the context and question.\n",
    "\n",
    "3. **Language Model**: Passes the structured prompt to the language model for generating a response.\n",
    "\n",
    "4. **Output Parsing**: Parses the language model's output into a clean string format using `StrOutputParser()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "080528c0-dd93-44ed-b23a-97389f2f8cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ffdb50-19ec-4bee-b8ad-e0ca824b0f19",
   "metadata": {},
   "source": [
    "### Step-10: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b873299-55d9-4633-be60-357f4e2cdf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This document describes the AGIEval benchmark and provides information about '\n",
      " 'the tasks included in it. \\n'\n",
      " '\\n'\n",
      " '\\n')\n"
     ]
    }
   ],
   "source": [
    "# Question\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(rag_chain.invoke(\"What is this document about?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "298a684f-65ac-4d39-8bb8-91e1e756b8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('System messages are hand-crafted prompts designed to elicit specific types '\n",
      " 'of responses from the language model. \\n'\n",
      " '\\n'\n",
      " 'There are 16 different system messages used in this research, each designed '\n",
      " 'to encourage the model to perform different tasks, such as:\\n'\n",
      " '\\n'\n",
      " '* Generating long and short answers\\n'\n",
      " '* Following instructions and formatting guidelines\\n'\n",
      " '* Creating creative content\\n'\n",
      " '* Answering information-seeking queries\\n'\n",
      " '* Providing explanations and step-by-step reasoning \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'These system messages are used to train the Orca model and influence the '\n",
      " 'types of responses it produces. \\n')\n"
     ]
    }
   ],
   "source": [
    "pprint(rag_chain.invoke(\"Explain about System messages?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d338b35-c4a1-4272-9169-a830f6dabeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The experiment setup included baselines and tasks. \\n'\n",
      " '\\n'\n",
      " 'The tasks were further divided into:\\n'\n",
      " '\\n'\n",
      " \"* **Open-ended Generation Capabilities:** This assessed the model's ability \"\n",
      " 'to generate creative and coherent text.\\n'\n",
      " \"* **Reasoning Capabilities:** This evaluated the model's ability to solve \"\n",
      " 'problems and understand logical relationships. \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n')\n"
     ]
    }
   ],
   "source": [
    "pprint(rag_chain.invoke(\"Explain about Experiment Setup?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b53bc25-f909-48b3-a90b-2f26dafed803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The provided context does not state who is to the right of P.  \\n'\n"
     ]
    }
   ],
   "source": [
    "#Out-of-Context Question\n",
    "\n",
    "pprint(rag_chain.invoke(\"Who is the Prime Minister\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp_ve",
   "language": "python",
   "name": "exp_ve"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
