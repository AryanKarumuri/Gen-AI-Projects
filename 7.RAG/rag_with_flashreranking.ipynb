{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf5fd1f-6d78-43ce-9859-ecbb09e05b9f",
   "metadata": {},
   "source": [
    "## FlashRank\n",
    "\n",
    "**[FlashRank](https://python.langchain.com/docs/integrations/retrievers/flashrank-reranker/)** is an ultra-lightweight and high-speed Python library designed to enhance search and retrieval pipelines with efficient **re-ranking** capabilities. It leverages **State-of-the-Art (SoTA)** cross-encoder models to improve search result relevance, making it an ideal choice for applications requiring accurate information retrieval.\n",
    "\n",
    "### Key Features:\n",
    "- **Fast and Lightweight**: Optimized for speed and minimal resource consumption.\n",
    "- **Seamless Integration**: Easily integrates with existing search pipelines.\n",
    "- **State-of-the-Art Models**: Utilizes advanced cross-encoders for high-quality re-ranking.\n",
    "- **Flexible and Scalable**: Supports various search and retrieval tasks across different domains.\n",
    "- **Community Acknowledgment**: Developed with appreciation for the AI community and the contributions of model creators.\n",
    "\n",
    "FlashRank is an excellent choice for developers looking to boost search performance without compromising on efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69328057-5d39-4259-b3cb-ae01ba25ee06",
   "metadata": {},
   "source": [
    "### Step-1: Required Package Installation\n",
    "\n",
    "These dependencies will set up a complete environment for working on a RAG system using Flash Reranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ccf87d0-5787-44d8-bf60-c7761e61a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flashrank langchain langchain_community huggingface-hub langchain-huggingface langchain-text-splitters faiss-cpu python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a80dc4-d037-42ca-af37-0b1f857e1c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers==2.2.2 InstructorEmbedding==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1829b1f7-dbe0-4af0-b8e4-b4376ad9936e",
   "metadata": {},
   "source": [
    "### Step-2: Imports\n",
    "\n",
    "These imports set up an environment for out tryout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1bcd053-ec87-43c3-84b4-0e1f34b503e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "import langchain\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877aedac-034f-4482-a014-995c66b9e232",
   "metadata": {},
   "source": [
    "### Step 3: LLM Setup\n",
    "\n",
    "In this step, we will be using the `llama-3.1-8b-instant` model from GROQ. To access and use the model, you will need to create an API key. \n",
    "Need steps for generate your API key, visit the following link: [GROQ API_Key_Generation](https://github.com/AryanKarumuri/Gen-AI-Projects/blob/main/README.md#api-key-generation-guide) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f26e2f-4933-468a-97b0-5bd3ec231d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if GROQ_API_KEY:\n",
    "    llm=ChatGroq(groq_api_key=GROQ_API_KEY,model_name=\"llama-3.1-8b-instant\")\n",
    "    print(GROQ_API_KEY)\n",
    "else:\n",
    "    print(\"Add Groq API Key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c316f401-3f74-4923-a02d-6baf8009e49f",
   "metadata": {},
   "source": [
    "### Step-4: Device Setup\n",
    "\n",
    "The `get_device()` function checks for the availability of different devices (**CUDA**, **XPU**, or **CPU**) and returns the appropriate device for computation.\n",
    "\n",
    "#### **Functionality**\n",
    "\n",
    "- **CUDA Availability Check**  \n",
    "  - The function first checks if a CUDA-capable GPU is available using `torch.cuda.is_available()`.  \n",
    "  - If CUDA is available, it selects the GPU as the device and prints the name of the GPU.  \n",
    "\n",
    "- **XPU Availability Check**  \n",
    "  - If CUDA is not available, the function checks for the presence of an XPU (Accelerator) using `torch.xpu.is_available()`.  \n",
    "  - If XPU is available, it selects the XPU device and clears the XPU cache using `torch.xpu.empty_cache()` to ensure no previous memory is used.  \n",
    "\n",
    "- **Fallback to CPU**  \n",
    "  - If neither CUDA nor XPU is available, the function defaults to using the **CPU** and prints `\"Using CPU\"`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce491432-bceb-4af3-88dd-892a3c3f2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"Check and return the appropriate device (XPU, CUDA, or CPU).\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device_type = \"cuda\"\n",
    "        device = torch.device(device_type)\n",
    "        print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    elif torch.xpu.is_available():\n",
    "        device_type = \"xpu\"\n",
    "        device = torch.device(device_type)\n",
    "        torch.xpu.empty_cache()  # Empty the XPU cache if using XPU\n",
    "        print(f\"Using device: {torch.xpu.get_device_name()}\")\n",
    "    else:\n",
    "        device_type = \"cpu\"\n",
    "        device = torch.device(device_type)\n",
    "        print(\"Using CPU\")\n",
    "        \n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21135939-3a40-4d77-9d1f-6bb7c3157621",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb60ee89-6011-405c-99a7-7ed1ead365dc",
   "metadata": {},
   "source": [
    "### Step-5: Doc Convertion\n",
    "\n",
    "`TextLoader` is designed specifically for loading plain text (.txt) files. It won't work directly with .docx files, as they contain additional formatting and are not plain text.\n",
    "\n",
    "The `convert_docx_to_txt()` function reads the contents of a `.docx` file and converts it into a plain text (`.txt`) file. This is useful when extracting text from Word documents for further processing or analysis.\n",
    "\n",
    "#### **Function Overview**\n",
    "- **Input:**  \n",
    "  - `docx_path` - Path to the input `.docx` file.  \n",
    "  - `txt_path` - Path to the output `.txt` file.  \n",
    "\n",
    "- **Output:**  \n",
    "  - A `.txt` file containing the extracted text from the Word document. Each paragraph is written to the text file on a new line.\n",
    "\n",
    "#### **Functionality**\n",
    "1. **Read DOCX File:**  \n",
    "    - The function uses `Document()` from the `python-docx` library to load the Word document.  \n",
    "\n",
    "2. **Write to TXT File:**  \n",
    "    - Opens the output text file in write mode (`\"w\"`) with UTF-8 encoding to ensure compatibility with various characters.  \n",
    "\n",
    "3. **Extract and Save Paragraphs:**  \n",
    "    - Iterates through all paragraphs in the `.docx` file using `doc.paragraphs`.  \n",
    "    - Writes each paragraphâ€™s text to the `.txt` file, followed by a newline (`\\n`) for formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "517e50cc-abf9-457f-b301-15c0b6fd5537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "def convert_docx_to_txt(docx_path, txt_path):\n",
    "    doc = Document(docx_path)\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for para in doc.paragraphs:\n",
    "            f.write(para.text + \"\\n\")\n",
    "\n",
    "convert_docx_to_txt(\"data/attention.docx\", \"attention.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4ec5eea-0e9d-4bc7-95cb-5cd34b243897",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = TextLoader(\"attention.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c677c92a-3553-4f59-94ab-c3ba51bf00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fbdcfb-7b9f-41be-b0e5-db0e5d0e28e8",
   "metadata": {},
   "source": [
    "### Step-6: Document Loading and Chunking\n",
    "\n",
    "- **`RecursiveCharacterTextSplitter`**: A text splitter is used to break down the loaded document into smaller chunks for easier processing by models. It splits the document into chunks of 1000 characters with a 300-character overlap between consecutive chunks.\n",
    "- **`text_splitter.split_documents(docs)`**: This line splits the loaded documents (`docs`) into chunks and stores them in the `document_chunks` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d37aa22-cf07-476b-af43-515a2215518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82edaf4-742e-4d8d-8091-4db75773a962",
   "metadata": {},
   "source": [
    "### Step-7: Getting Embeddings\n",
    "\n",
    "- **`EMBEDDING_MODEL_NAME`**: Specifies the name of the pre-trained embedding model, which is `\"hkunlp/instructor-large\"`. This model is designed for generating embeddings.\n",
    "\n",
    "- **`get_embeddings()`**: Initializes the `HuggingFaceInstructEmbeddings` with the given model name and provides instructions for how to represent documents and queries for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f984c0e9-df4b-4aa6-a19a-d2813616954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained Embedding Model.\n",
    "EMBEDDING_MODEL_NAME = \"hkunlp/instructor-large\"\n",
    "\n",
    "#Embeddings\n",
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL_NAME,\n",
    "            embed_instruction=\"Represent the document for retrieval:\",\n",
    "            query_instruction=\"Represent the question for retrieving supporting documents:\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb73ea6-cf19-4f88-85d3-7339118d8ab7",
   "metadata": {},
   "source": [
    "### Step-8: DB Retriever\n",
    "\n",
    "- **`FAISS.from_documents(texts, embeddings)`**  \n",
    "  - Creates a FAISS index from a collection of documents (`texts`) and their corresponding vector representations (`embeddings`).  \n",
    "  - FAISS is an efficient similarity search library used for searching in large datasets of vector embeddings.  \n",
    "\n",
    "- **`.as_retriever()`**  \n",
    "  - Converts the FAISS index into a retriever object, making it easy to query for relevant documents using similarity search.  \n",
    "\n",
    "- **`search_kwargs={\"k\": 7}`**  \n",
    "  - Specifies that the retriever should return the **top 7 most relevant documents** for each query, based on similarity scores.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7785ef0-b76a-46bd-90d0-767ea8526637",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = FAISS.from_documents(texts, embeddings).as_retriever(search_kwargs={\"k\": 7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f59c390-4749-4482-8dc1-7ccc823e0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is multi-head attention?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fcdb828-0d53-4f2a-8e40-9f9513aed70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c9a8055-7916-4ab7-93c1-6568b52b8f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './attention.txt'}, page_content='MultiHead(Q, K, V ) = Concat(head1, ..., headh)WO\\nwhere headi = Attention(QWQ, KWK, V WV )\\ni\\ti\\ti\\n\\n\\nWhere the projections are parameter matrices WQ âˆˆ RdmodelÃ—dk , WK âˆˆ RdmodelÃ—dk , WV âˆˆ RdmodelÃ—dv\\n\\nand WO âˆˆ Rhdv Ã—dmodel .\\ni\\ti\\ti\\n\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.'),\n",
       " Document(metadata={'source': './attention.txt'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\n\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)WO\\nwhere headi = Attention(QWQ, KWK, V WV )\\ni\\ti\\ti'),\n",
       " Document(metadata={'source': './attention.txt'}, page_content='Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional'),\n",
       " Document(metadata={'source': './attention.txt'}, page_content='Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n\\nScaled Dot-Product Attention\\tMulti-Head Attention\\n\\n\\n\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.'),\n",
       " Document(metadata={'source': './attention.txt'}, page_content='Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\nIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].'),\n",
       " Document(metadata={'source': './attention.txt'}, page_content='Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This'),\n",
       " Document(metadata={'source': './attention.txt'}, page_content='Encoder:  The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f80d016-e74c-4417-a11d-727ef711e6ec",
   "metadata": {},
   "source": [
    "### Description:\n",
    "\n",
    "The `pretty_print_docs()` function neatly displays a list of documents by printing their content and metadata in a formatted manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c51c36f8-c722-4c84-be7f-a8bf56719d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [\n",
    "                f\"Document {i+1}:\\n\\n{d.page_content}\\nMetadata: {d.metadata}\"\n",
    "                for i, d in enumerate(docs)\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba5bc4c8-75e6-4d41-bfa4-500badccb633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "MultiHead(Q, K, V ) = Concat(head1, ..., headh)WO\n",
      "where headi = Attention(QWQ, KWK, V WV )\n",
      "i\ti\ti\n",
      "\n",
      "\n",
      "Where the projections are parameter matrices WQ âˆˆ RdmodelÃ—dk , WK âˆˆ RdmodelÃ—dk , WV âˆˆ RdmodelÃ—dv\n",
      "\n",
      "and WO âˆˆ Rhdv Ã—dmodel .\n",
      "i\ti\ti\n",
      "\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "Metadata: {'source': './attention.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "\n",
      "MultiHead(Q, K, V ) = Concat(head1, ..., headh)WO\n",
      "where headi = Attention(QWQ, KWK, V WV )\n",
      "i\ti\ti\n",
      "Metadata: {'source': './attention.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "Metadata: {'source': './attention.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "\n",
      "Scaled Dot-Product Attention\tMulti-Head Attention\n",
      "\n",
      "\n",
      "\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
      "Metadata: {'source': './attention.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\n",
      "Metadata: {'source': './attention.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "Metadata: {'source': './attention.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "\n",
      "Encoder:  The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate\n",
      "Metadata: {'source': './attention.txt'}\n"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9840bfaa-ae42-4181-8716-4251a62dff97",
   "metadata": {},
   "source": [
    "### Step-9: ContextualCompressionRetriever and FlaskReranker Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc0004-2eda-4505-ab98-1d1be9863c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = FlashrankRerank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7086b7b-1e12-4534-8fc9-70d4fae311e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "195e9d07-40b4-40d9-8ea4-6fdae1118ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_docs = compression_retriever.invoke(\"What is multi-head attention?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e28ad7ca-c2ec-467c-b544-44cac5dd43e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 3, 'relevance_score': 0.9992928, 'source': './attention.txt'}, page_content='Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n\\nScaled Dot-Product Attention\\tMulti-Head Attention\\n\\n\\n\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.'),\n",
       " Document(metadata={'id': 2, 'relevance_score': 0.99906945, 'source': './attention.txt'}, page_content='Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional'),\n",
       " Document(metadata={'id': 5, 'relevance_score': 0.9989217, 'source': './attention.txt'}, page_content='Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75ab71f2-4a0b-4eca-9034-9270860df235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "\n",
      "Scaled Dot-Product Attention\tMulti-Head Attention\n",
      "\n",
      "\n",
      "\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
      "Metadata: {'id': 3, 'relevance_score': 0.9992928, 'source': './attention.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "Metadata: {'id': 2, 'relevance_score': 0.99906945, 'source': './attention.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "Metadata: {'id': 5, 'relevance_score': 0.9989217, 'source': './attention.txt'}\n"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79197494-639a-446b-825d-46795de60d30",
   "metadata": {},
   "source": [
    "### Step-10: Chain Setup\n",
    "The code uses a **RetrievalQA** chain to perform a question-answering task using a compressed retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "944725b4-de0b-495f-aa84-be7b4aeec281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 09:54:23,250 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is multi-head attention?',\n",
       " 'result': 'Multi-head attention is a type of attention mechanism used in transformer models, particularly in natural language processing and machine translation tasks. It\\'s an extension of the scaled dot-product attention mechanism.\\n\\nIn traditional attention, the attention weights are computed by taking the dot product of the query and key vectors, and then applying a scaling factor and a softmax function to get the final weights.\\n\\nMulti-head attention, on the other hand, linearly projects the query, key, and value vectors into multiple subspaces (or \"heads\") of dimensionality `dk`, `dk`, and `dv` respectively, where `h` is the number of heads. Each head computes attention weights independently, and the final output is a weighted sum of all the heads.\\n\\nThis allows the model to jointly attend to information from different representation subspaces at different positions. The intuition behind multi-head attention is that different heads can focus on different aspects of the input, and the model can learn to combine these different aspects to form a more robust representation.\\n\\nMulti-head attention has been shown to improve the performance of transformer models in various tasks, including machine translation, text classification, and question answering.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, retriever=compression_retriever)\n",
    "chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec454802-85d9-4048-b000-ba914f37ec10",
   "metadata": {},
   "source": [
    "### Below is the answer that is out-of-context. It is just givivng the answer based on relevance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e777a855-7ee3-4316-b14a-8b0296fc6eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\n",
      "\n",
      "Attention Is All You Need\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ashish Vaswaniâˆ— Google Brain avaswani@google.com\n",
      "Noam Shazeerâˆ— Google Brain noam@google.com\n",
      "Niki Parmarâˆ— Google Research nikip@google.com\n",
      "Jakob Uszkoreitâˆ— Google Research usz@google.com\n",
      "Metadata: {'id': 0, 'relevance_score': 0.9969857, 'source': './attention.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "âˆ—Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "Metadata: {'id': 3, 'relevance_score': 0.93468195, 'source': './attention.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n",
      "\n",
      "Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\n",
      "Advances in Neural Information Processing Systems, 2015.\n",
      "Metadata: {'id': 4, 'relevance_score': 0.029318662, 'source': './attention.txt'}\n"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(compression_retriever.invoke(\"Who is the prime minister of India?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c34faa-4fa0-4bf4-91c8-5e343331be08",
   "metadata": {},
   "source": [
    "### Below is the best practical example for using llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ca6ebbe-0215-4fbe-8354-8973a7a9b37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 10:17:51,874 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Who is the prime minister of India?',\n",
       " 'result': \"I don't have the most current information on the prime minister of India. My knowledge cutoff is December 2023, and I may not have information on recent changes or updates. For the most accurate and up-to-date information, I recommend checking a reliable news source or official government website.\"}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who is the prime minister of India?\"\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, retriever=compression_retriever)\n",
    "chain.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp_ve",
   "language": "python",
   "name": "exp_ve"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
