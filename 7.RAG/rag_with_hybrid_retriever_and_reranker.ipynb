{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0be1ddf-0193-4435-acea-274766d70c28",
   "metadata": {},
   "source": [
    "# RAG using Hybrid Retrievers and Reranking\n",
    "\n",
    "**Introduction to RAG using Hybrid Retrievers and Reranking:**\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a framework where language models (LMs) are combined with information retrieval systems to improve the model's ability to generate contextually accurate and informative responses. Hybrid retrievers combine multiple retrieval methods (e.g., sparse and dense retrieval) to obtain relevant documents, which are then passed through a reranking system to prioritize the most useful information. This hybrid approach boosts the performance of LMs by ensuring they work with the most relevant external data, leading to more accurate and coherent outputs.\n",
    "\n",
    "## Advantages:\n",
    "\n",
    "- **Improved accuracy**: Combines multiple retrieval techniques for a more comprehensive set of relevant documents.\n",
    "- **Efficient use of resources**: Leverages both dense and sparse retrieval, enhancing coverage without excessive computation.\n",
    "- **Better context understanding**: Reranking ensures the model focuses on the most relevant and high-quality data for generation.\n",
    "- **Reduced hallucination**: Access to real-world knowledge decreases the likelihood of generating factually incorrect information.\n",
    "\n",
    "## Disadvantages:\n",
    "\n",
    "- **Increased complexity**: Requires more sophisticated architecture and tuning.\n",
    "- **Dependency on quality of retrieval**: Performance heavily depends on the quality of the documents retrieved.\n",
    "- **Computational cost**: Reranking and hybrid retrieval can lead to higher computational overhead.\n",
    "- **Limited by retriever quality**: If the retriever doesn't fetch relevant data, the generated output might still be subpar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0a642d-f66d-4ba1-b7a8-445f2143645f",
   "metadata": {},
   "source": [
    "### Step-1: Required Package Installation\n",
    "\n",
    "These dependencies will set up a complete environment for working on a RAG system using Langchain, along with embeddings, document retrieval, and generative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910d038-27b4-48d7-bed8-317e8d9f88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dotenv transformers==4.44.2 langchain==0.3.3 \\\n",
    "                             langchain-community==0.3.0 \\\n",
    "                             langchain-core==0.3.10 \\\n",
    "                             langchain-text-splitters==0.3.0 \\\n",
    "                            chroma-hnswlib==0.7.6 \\\n",
    "                             chromadb==0.5.11 \\\n",
    "                             accelerate==1.0.1 \\\n",
    "                             pypdf \\\n",
    "                             ipywidgets \\\n",
    "                            langchain-groq \\\n",
    "                            huggingface-hub==0.25.1 \\\n",
    "                            langchain-huggingface==0.1.0 \\\n",
    "                            InstructorEmbedding==1.0.1 \\\n",
    "                             rank-bm25==0.2.2\n",
    "!pip install sentence-transformers==2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fc8b6c-adfe-4f75-a404-0e93639a0b16",
   "metadata": {},
   "source": [
    "### Step-2: Imports\n",
    "\n",
    "These imports set up an environment that integrates document loading, embeddings, vector stores, and interactions with large language models (LLMs), making it suitable for building RAG (Retrieval-Augmented Generation) systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b568dd8-63d7-4507-bc3e-650054d4e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from dotenv import load_dotenv\n",
    "import bs4\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Chroma and related imports\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Langchain related imports\n",
    "import langchain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f18ad-b2c8-4874-bf20-f18040bdee00",
   "metadata": {},
   "source": [
    "### Step 3: LLM Setup\n",
    "\n",
    "In this step, we will be using the `llama-3.1-8b-instant` model from GROQ. To access and use the model, you will need to create an API key. \n",
    "Need steps for generate your API key, visit the following link: [GROQ API_Key_Generation](https://github.com/AryanKarumuri/Gen-AI-Projects/blob/main/README.md#api-key-generation-guide) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab0dd4-c9fc-441a-81ac-5f23ee0eb830",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if GROQ_API_KEY:\n",
    "    llm=ChatGroq(groq_api_key=GROQ_API_KEY,model_name=\"llama-3.1-8b-instant\")\n",
    "    #print(GROQ_API_KEY)\n",
    "else:\n",
    "    print(\"Add Groq API Key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0426ecb3-0dbb-48b5-8b56-50645e612d8a",
   "metadata": {},
   "source": [
    "### Step-4: Data Loading\n",
    "\n",
    "The provided code uses the `WebBaseLoader` to scrape and load content from a specified web page. In this case, it fetches content from a post titled \"hallucination\" on Lilian Weng's website.\n",
    "\n",
    "- **web_paths**: Defines the URL to scrape (`https://lilianweng.github.io/posts/2024-07-07-hallucination/`).\n",
    "- **bs_kwargs**: Passes additional parameters to BeautifulSoup for filtering the content, specifically extracting elements with classes like `post-content`, `post-title`, and `post-header`.\n",
    "  \n",
    "The code will load the relevant sections of the page based on these filters using `loader.load()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c7f9f94-42dd-4320-8ce5-1b7e4c78fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5e099c-9f4f-4b59-8ae9-323ebe8046aa",
   "metadata": {},
   "source": [
    "### Step-5: Document Loading and Chunking\n",
    "\n",
    "This code first checks whether any documents were successfully loaded. If no documents are found, it raises a `ValueError`.\n",
    "\n",
    "- **`RecursiveCharacterTextSplitter`**: A text splitter is used to break down the loaded document into smaller chunks for easier processing by models. It splits the document into chunks of 1000 characters with a 300-character overlap between consecutive chunks.\n",
    "- **`text_splitter.split_documents(docs)`**: This line splits the loaded documents (`docs`) into chunks and stores them in the `document_chunks` list\n",
    "\n",
    "This approach is useful for large documents that need to be processed in smaller pieces to fit within model input size limitations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92846027-f720-4e80-b93d-6f1c79a56c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Extrinsic Hallucinations in LLMs\n",
      "    \n",
      "Date: July 7, 2024  |  Estimated Reading Time: 30 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\n",
      "There are two types of hallucination:' metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/'}\n"
     ]
    }
   ],
   "source": [
    "if not docs:\n",
    "    raise ValueError(\"No documents loaded.\")\n",
    "\n",
    "# Splitting docs into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=300)\n",
    "document_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(document_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d58a55-5f56-48ec-9cb4-595d502265aa",
   "metadata": {},
   "source": [
    "### Step-7: Post-Processing\n",
    "\n",
    "- **`format_docs(docs)`**: The function takes a list of documents (`docs`) and joins their `page_content` into a single string. The content is separated by two newline characters (`\\n\\n`) for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7398104e-159e-4f8e-b9b4-33c2b8776429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e275fe-57c7-4347-afa1-389ef5620cfa",
   "metadata": {},
   "source": [
    "### Step-8: Getting Embeddings\n",
    "\n",
    "- **`EMBEDDING_MODEL_NAME`**: Specifies the name of the pre-trained embedding model, which is `\"hkunlp/instructor-large\"`. This model is designed for generating embeddings.\n",
    "\n",
    "- **`get_embeddings()`**: Initializes the `HuggingFaceInstructEmbeddings` with the given model name and provides instructions for how to represent documents and queries for retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2178dba3-e8ba-4db1-8455-2aa2a886876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained Embedding Model.\n",
    "EMBEDDING_MODEL_NAME = \"hkunlp/instructor-large\"\n",
    "\n",
    "#Embeddings\n",
    "def get_embeddings():\n",
    "    embeddings = HuggingFaceInstructEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL_NAME,\n",
    "            embed_instruction=\"Represent the document for retrieval:\",\n",
    "            query_instruction=\"Represent the question for retrieving supporting documents:\"\n",
    "        )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f240b3f-2afb-42f1-a95c-45306ac21617",
   "metadata": {},
   "source": [
    "### Step-9: Database Creation and DB Retriever\n",
    "\n",
    "- **Database Creation** (`Chroma.from_documents()`): Creates a vector store using **Chroma**, which indexes the document chunks and stores their embeddings in a collection.\n",
    "   \n",
    "- **DB Retriever** (`vector_store.as_retriever()`): Converts the vector store into a retriever, allowing for efficient querying and retrieval of relevant documents based on vector similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b9706-8c04-49d4-876e-561411341944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Creation\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=document_chunks,       \n",
    "    embedding=get_embeddings(),    \n",
    "    collection_name=\"db_collection\"  \n",
    ")\n",
    "\n",
    "# DB Retriever\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc07e1b-3bbf-4f17-b022-5d8a1e71d2ee",
   "metadata": {},
   "source": [
    "### Step-10: Hybrid Retrievers Setup\n",
    "\n",
    "This function defines a **hybrid retriever system** that combines multiple retrieval techniques to improve the quality of document retrieval. Here's a breakdown of each component:\n",
    "\n",
    "1. **DB Retriever**: \n",
    "   - A retriever is created from a vector store (`vector_store.as_retriever()`). This typically represents a database of vectors (dense representations of documents) for semantic search.\n",
    "     \n",
    "\n",
    "2. **BM25 Retriever**: \n",
    "   - A **BM25 retriever** is initialized using the document chunks. BM25 is a sparse retrieval method commonly used for document ranking based on term frequency.\n",
    "   - The number of top results (`k`) to return is set to 5.\n",
    "     \n",
    "\n",
    "3. **Contextual Compression Retriever**:\n",
    "   - A **HuggingFaceCrossEncoder** is used for reranking based on a pre-trained model (`BAAI/bge-reranker-base`).\n",
    "   - The **CrossEncoderReranker** is used for compressing and reranking the documents, selecting the top 3.\n",
    "   - The **ContextualCompressionRetriever** combines the compression model with the original retriever, adding context-sensitive compression to the retrieval process.\n",
    "\n",
    "\n",
    "4. **Ensemble Retriever**:\n",
    "   - An **EnsembleRetriever** is created by combining the **ContextualCompressionRetriever** and the **BM25 Retriever**. The weights (0.7 and 0.3) indicate the contribution of each retriever to the final ranking.\n",
    "     \n",
    "\n",
    "The function returns the **ensemble retriever**, which will leverage both dense and sparse retrieval techniques to improve the accuracy and relevance of the search results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "574f6cd7-f7ca-4348-9c64-ecaecfc338c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_retrievers(document_chunks):\n",
    "    # DB Retriever\n",
    "    retriever = vector_store.as_retriever()\n",
    "    \n",
    "    # BM25 Retriever\n",
    "    sparse_retriever = BM25Retriever.from_documents(document_chunks)\n",
    "    sparse_retriever.k = 5\n",
    "\n",
    "    # Adding Contextual Compression Retriever\n",
    "    model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\")\n",
    "    compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=retriever\n",
    "    )\n",
    "\n",
    "    # Creating Ensemble Retriever\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[compression_retriever, sparse_retriever],\n",
    "        weights=[0.7, 0.3]\n",
    "    )\n",
    "    \n",
    "    return ensemble_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0fe98-729a-436f-8994-d4c8de24aa23",
   "metadata": {},
   "source": [
    "### Step-11: System Prompt\n",
    "\n",
    "1. **System Prompt**: Defines the behavior of the assistant for question-answering tasks. It ensures that the assistant answers strictly based on the provided context and informs the user when the answer is unknown.\n",
    "\n",
    "2. **Prompt Template** (`ChatPromptTemplate.from_template()`): Creates a flexible prompt template using the system prompt. It replaces the placeholders `{context}` and `{question}` with actual values at runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c8c9391-9896-4a74-910b-0d29d5741577",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer the question. \"\n",
    "        \"You must answer questions strictly using the provided context. If you don't know the answer, say that you don't know. \"\n",
    "        \"\\n\\n\"\n",
    "        \"{context}\"\n",
    "        \"Question: {question}\"\n",
    "    )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee2d6a5",
   "metadata": {},
   "source": [
    "### Step-12: RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "078aaa8b-f8fe-4a5d-b5e0-6da13b5fb524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": hybrid_retrievers(document_chunks) | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b05786b",
   "metadata": {},
   "source": [
    "### Testing our Hybrid RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37af9fbb-32b8-4c0c-985c-c4068fb07d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Based on the provided context, I can answer your question:\\n'\n",
      " '\\n'\n",
      " 'What is this document about?\\n'\n",
      " '\\n'\n",
      " 'According to the context, this document is about Anti-Hallucination Methods '\n",
      " 'for Language Models (LLMs), specifically discussing various methods to '\n",
      " 'improve factuality and reduce hallucinations in LLMs, including '\n",
      " 'Retrieval-augmented Generation (RAG), RARR, WebGPT, and fine-tuning for '\n",
      " 'attribution and factuality.')\n"
     ]
    }
   ],
   "source": [
    "# Question\n",
    "from pprint import pprint\n",
    "pprint(rag_chain.invoke(\"What is this document about?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44b895fd-4289-4a36-9234-b5af2e9b8fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('FactualityPrompt is a benchmark consisting of both factual and nonfactual '\n",
      " 'prompts, relying on Wikipedia documents or sentences as the knowledge base '\n",
      " 'for factuality grounding.')\n"
     ]
    }
   ],
   "source": [
    "pprint(rag_chain.invoke(\"What is FactualityPrompt?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f73e39a-6a90-4ce8-beda-fb91642f5edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('According to the provided context, some interesting observations on model '\n",
      " 'hallucination behavior are as follows:\\n'\n",
      " '\\n'\n",
      " '1. Error rates are higher for rarer entities in the task of biography '\n",
      " 'generation.\\n'\n",
      " '2. Error rates are higher for facts mentioned later in the generation.\\n'\n",
      " '3. Using retrieval to ground the model generation significantly helps reduce '\n",
      " 'hallucination.\\n'\n",
      " '\\n'\n",
      " 'Additionally, there are some observations from the experiments, where dev '\n",
      " 'set accuracy is considered a proxy for hallucinations. These include:\\n'\n",
      " '\\n'\n",
      " '1. Unknown examples are fitted substantially slower than Known examples.\\n'\n",
      " '2. The best dev performance is obtained when the LLM fits the majority of '\n",
      " 'the Known training examples but only a few of the Unknown ones. The model '\n",
      " 'starts to hallucinate when it learns most of the Unknown examples.\\n'\n",
      " '3. Among Known examples, MaybeKnown cases result in better overall '\n",
      " 'performance, more essential than HighlyKnown ones.\\n'\n",
      " '\\n'\n",
      " 'These observations highlight some of the factors that contribute to model '\n",
      " 'hallucination behavior and possible strategies to mitigate it.')\n"
     ]
    }
   ],
   "source": [
    "pprint(rag_chain.invoke(\"What are the observations on model hallucination behavior?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e64ab438-9613-4445-b765-e62830ef668a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Based on the provided context, I will explain about Self-RAG.\\n'\n",
      " '\\n'\n",
      " 'Self-RAG, also known as \"Self-reflective retrieval-augmented generation,\" is '\n",
      " 'a framework that trains a Language Model (LM) end-to-end to learn to reflect '\n",
      " 'on its own generation. It achieves this by outputting both task output and '\n",
      " 'intermittent special reflection tokens.\\n'\n",
      " '\\n'\n",
      " 'The Self-RAG model uses four types of reflection tokens: \\n'\n",
      " '\\n'\n",
      " '1. Retrieve: decides whether to run retrieval in parallel to get a set of '\n",
      " 'documents.\\n'\n",
      " '2. IsRel: whether the prompt and retrieved document are relevant.\\n'\n",
      " '3. IsSup: whether the output text is supported by the retrieved document.\\n'\n",
      " '4. IsUse: whether the output text is useful to the given prompt.\\n'\n",
      " '\\n'\n",
      " 'Self-RAG trains a critic model and a generator model by prompting GPT-4 and '\n",
      " 'then distills that into an in-house model to reduce inference cost.\\n'\n",
      " '\\n'\n",
      " 'Self-RAG aims to improve the quality of the generated output by critiquing '\n",
      " 'its own generation through self-reflection.')\n"
     ]
    }
   ],
   "source": [
    "pprint(rag_chain.invoke(\"Explain about self RAG from the document.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d170ee3-595e-4d61-93a6-ab20baa295a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The provided context mentions the formula for sampling methods in the '\n",
      " 'context of factual-nucleus sampling, which is as follows:\\n'\n",
      " '\\n'\n",
      " '\\\\[ p_t = \\\\max(\\\\omega, p \\\\cdot \\\\lambda^{t−1}) \\\\]\\n'\n",
      " '\\n'\n",
      " 'where:\\n'\n",
      " '\\n'\n",
      " '- $p_t$ is the probability at the $t$-th token in the sentence.\\n'\n",
      " '- $\\\\omega$ is a parameter to prevent the sampling from falling back to '\n",
      " 'greedy sampling, which can hurt generation quality and diversity.\\n'\n",
      " '- $p$ is the initial probability.\\n'\n",
      " '- $\\\\lambda$ is a parameter that dynamically adapts the probability during '\n",
      " 'sampling.')\n"
     ]
    }
   ],
   "source": [
    "pprint(rag_chain.invoke(\"Explain the formula for sampling methods.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp_ve",
   "language": "python",
   "name": "exp_ve"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
