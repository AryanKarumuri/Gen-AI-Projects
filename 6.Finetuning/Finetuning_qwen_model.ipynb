{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc177166-39d5-4444-ae18-77445e4b696e",
   "metadata": {},
   "source": [
    "## 1. Importing Required LibrariesðŸ“¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4a1445-8c30-499d-9d9b-ce5fc0ccd9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import site\n",
    "import os\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "num_physical_cores = psutil.cpu_count(logical=False)\n",
    "num_cores_per_socket = num_physical_cores // 2\n",
    "\n",
    "print(\"Number of physical cores: \", num_physical_cores)\n",
    "print(\"Number of cores per socket: \", num_cores_per_socket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f9000a-fa7f-4553-af7e-d4fd46fe380a",
   "metadata": {},
   "source": [
    "## 2. Device SetupðŸ”§ðŸ’»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d59bfec3-ef8e-47f5-9637-81d24aec75e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"Check and return the appropriate device (XPU, CUDA, or CPU).\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device_type = \"cuda\"\n",
    "        device = torch.device(device_type)\n",
    "        print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    elif torch.xpu.is_available():\n",
    "        device_type = \"xpu\"\n",
    "        device = torch.device(device_type)\n",
    "        torch.xpu.empty_cache()  # Empty the XPU cache if using XPU\n",
    "        print(f\"Using device: {torch.xpu.get_device_name()}\")\n",
    "    else:\n",
    "        device_type = \"cpu\"\n",
    "        device = torch.device(device_type)\n",
    "        print(\"Using CPU\")\n",
    "        \n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac224b-726f-4e9f-910b-ec98817110a9",
   "metadata": {},
   "source": [
    "- **CUDA Availability Check:** The function first checks if a CUDA-capable GPU is available using torch.cuda.is_available(). If CUDA is available, it selects the GPU as the device and prints the name of the GPU.\n",
    "\n",
    "- **XPU Availability Check:** If CUDA is not available, the function checks if an XPU (Accelerator) is available using torch.xpu.is_available(). If XPU is available, it selects the XPU device and empties the XPU cache to ensure no previous memory is used.\n",
    "\n",
    "- **Fallback to CPU:** If neither CUDA nor XPU is available, the function defaults to using the CPU as the device and prints \"Using CPU\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dea2e2-bb06-43ee-a12d-0f6328a50ba8",
   "metadata": {},
   "source": [
    "## 3. Setting up LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72846b7e-aeb3-4f1f-b422-6d8167abb13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    # could use q, v and 0 projections as well and comment out the rest\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \n",
    "                    \"v_proj\", \"k_proj\", \n",
    "                    \"gate_proj\", \"up_proj\",\n",
    "                    \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b92b4ca-b438-4221-a002-0d10753a86f8",
   "metadata": {},
   "source": [
    "### Explanation of `LoraConfig` Parameters\n",
    "\n",
    "-  **r=32**\n",
    "   - **What it is**: The rank (`r`) represents how many dimensions the low-rank adaptation uses. It controls the size of the updates to the modelâ€™s parameters.\n",
    "   - **Why it matters**: A higher rank allows the model to adapt more flexibly, but it also uses more memory and processing power. In this case, `r=32` means the model can use a moderate amount of flexibility in adapting to new tasks.\n",
    "\n",
    "-  **lora_alpha=16**\n",
    "   - **What it is**: This is a scaling factor for the low-rank updates.\n",
    "   - **Why it matters**: It controls how strongly the low-rank updates affect the modelâ€™s parameters. A higher value (like `16`) means the updates will have a bigger impact on the model, making the adaptation stronger.\n",
    "\n",
    "-  **lora_dropout=0.1**\n",
    "   - **What it is**: Dropout is a technique used during training to randomly ignore some parts of the model to prevent overfitting.\n",
    "   - **Why it matters**: Here, `lora_dropout=0.1` means that during training, 10% of the low-rank connections will be randomly turned off. This helps the model generalize better to new data.\n",
    "\n",
    "-  **bias=\"none\"**\n",
    "   - **What it is**: This setting controls whether bias terms are added to the low-rank updates.\n",
    "   - **Why it matters**: Setting `bias=\"none\"` means no additional bias is added, simplifying the model. This focuses the adaptation on low-rank updates, making it computationally lighter.\n",
    "\n",
    "-  **target_modules=[\"q_proj\", \"o_proj\", \"v_proj\", \"k_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]**\n",
    "   - **What it is**: This is a list of specific layers in the model where the low-rank updates will be applied.\n",
    "   - **Why it matters**: These layers are parts of the attention mechanism and other model components. By applying low-rank updates to only certain layers (like `q_proj`, `v_proj`, etc.), we can fine-tune the model more efficiently without touching everything. Each of these components has a specific role in processing data:\n",
    "     - **q_proj**: Handles the \"query\" in attention, which helps the model look at previous words to predict the next one.\n",
    "     - **o_proj**: Manages the output after attention.\n",
    "     - **v_proj**: Deals with the \"values\" in attention, which represent the information the model is focusing on.\n",
    "     - **k_proj**: Handles the \"keys\" in attention, which help match queries with values.\n",
    "     - **gate_proj**: Often controls or adjusts the flow of information in the model.\n",
    "     - **up_proj**: Used to expand the feature size in some parts of the model.\n",
    "     - **down_proj**: Used to reduce the feature size in other parts of the model.\n",
    "\n",
    "-  **task_type=\"CAUSAL_LM\"**\n",
    "   - **What it is**: This defines the type of task the model is being adapted for.\n",
    "   - **Why it matters**: Setting this to `\"CAUSAL_LM\"` means the model is being trained for causal language modeling. In this task, the model predicts the next word based only on the words before it (not the ones after). This is common in models like GPT that generate text one word at a time.\n",
    "\n",
    "### Summary:\n",
    "The `LoraConfig` allows you to fine-tune a pre-trained model by applying low-rank adaptations to specific layers, which helps the model learn more efficiently for new tasks. You can control the strength of these updates, how much the model \"ignores\" during training to prevent overfitting, and which parts of the model to focus on for adapting to the task (like predicting the next word in a sequence).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03821f6c-8241-444a-9979-c77c18a50ae5",
   "metadata": {},
   "source": [
    "## 4. Model Initialization and Optimization for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db50a357-5f8e-4fc7-bdee-7d1f5b04a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#Model ID\n",
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set padding side to the right to ensure proper attention masking during fine-tuning\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load the model and move it to the appropriate device\n",
    "device = get_device()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "\n",
    "# Disable caching mechanism to reduce memory usage during fine-tuning\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Configure the model's pre-training tensor parallelism degree to match the fine-tuning setup\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Enable gradient checkpointing to save memory during backpropagation\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Enable mixed precision for reduced memory usage and faster computation\n",
    "model.fp16 = True  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e6210e-359a-438c-aabf-deb3fd072a9a",
   "metadata": {},
   "source": [
    "### Explanation of Key Configurations\n",
    "\n",
    "- **Setting the Padding Token**: \n",
    "  - The `pad_token` is set to the `eos_token` (End of Sequence token). This is done to handle padding in the input sequences during training, ensuring the tokenizer uses the EOS token as padding.\n",
    "\n",
    "- **Padding Side**: \n",
    "  - Sets the padding side of the tokenizer to `\"right\"`. This ensures that when padding is added to sequences, it happens at the end (right side), which is important for attention masking during fine-tuning.\n",
    "\n",
    "- **Disabling Cache**: \n",
    "  - This setting is turned off to save memory during fine-tuning. By default, caching speeds up inference, but it uses additional memory. Disabling it reduces memory usage during training.\n",
    "\n",
    "- **Tensor Parallelism**: \n",
    "  - This sets the degree of tensor parallelism used during pre-training. A value of `1` means no parallelism (default setup). If you are using multiple GPUs for pre-training, you can adjust this value to split the tensor computation across GPUs.\n",
    "\n",
    "- **Gradient Checkpointing**: \n",
    "  - This feature is enabled to reduce memory usage during backpropagation (the process of updating model weights). It saves memory by not storing intermediate activations and recomputing them during backpropagation, which is especially useful for large models.\n",
    "\n",
    "- **Mixed Precision (FP16)**: \n",
    "  - Enables mixed precision training, which uses 16-bit floating point numbers (half precision) instead of the default 32-bit floating point (single precision). This reduces memory usage and speeds up computation, especially when fine-tuning large models on GPUs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca23f9a-e256-4db9-b839-d692dfc4f674",
   "metadata": {},
   "source": [
    "## 5.Testing the Base Model\n",
    "\n",
    "Let's get answers for the questions from the base model(i.e., not finetuned model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a637f-3063-4fec-aaeb-0388a6f96086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)    \n",
    "    outputs = model.generate(input_ids, max_new_tokens=250,\n",
    "                             eos_token_id=tokenizer.eos_token_id)    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def test_model(model, test_inputs):\n",
    "    \"\"\"quickly test the model using queries.\"\"\"\n",
    "    for input_text in test_inputs:\n",
    "        print(\"__\"*25)\n",
    "        generated_response = generate_response(model, input_text)\n",
    "        print(f\"{input_text}\")\n",
    "        print(f\"Generated Answer: {generated_response}\\n\")\n",
    "        print(\"__\"*25)\n",
    "\n",
    "test_inputs = [\n",
    "    \"Who are the authors of the paper-Attention all you need?\",\n",
    "    \"List out the formulas given the paper.\",\n",
    "    \"What is llama?\",\n",
    "    \"What are the different types of finetuning techniques?\",\n",
    "    \"What is Gen-AI?\",\n",
    "    \"What are the difference between Machine Learning algorithms and AI algorithms?\"\n",
    "]\n",
    "\n",
    "print(\"Testing the model before fine-tuning:\")\n",
    "test_model(model, test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1572f0dd-b3ba-42de-8e40-74efe103440f",
   "metadata": {},
   "source": [
    "## Dataset Loading, Filtering, and Preprocessing\n",
    "\n",
    "This script demonstrates the process of loading, filtering, and preprocessing a dataset using the Hugging Face `datasets` library. Specifically, it works with the `databricks/databricks-dolly-15k` dataset, which is a collection of instruction-response pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ca111-acc5-46c2-8e94-bb4c679f6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "raw_data = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Display a summary of the raw dataset\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9232fa8-e323-4fe2-83d9-fc829f1ce7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample instruction and response\n",
    "print(f\"Instruction: {raw_data[0]['instruction']}\")\n",
    "print(f\"Response: {raw_data[0]['response']}\")\n",
    "\n",
    "# Filter dataset to keep only question-answer categories\n",
    "qa_categories = {\"close_qa\", \"open_qa\", \"general_qa\"}\n",
    "qa_data = raw_data.filter(lambda example: example['category'] in qa_categories)\n",
    "\n",
    "# Display filtered dataset information\n",
    "print(f\"Filtered dataset contains {len(qa_data)} examples.\")\n",
    "print(f\"Categories in filtered dataset: {qa_data['category'][:10]}\")\n",
    "\n",
    "# Remove unnecessary fields for a cleaner dataset\n",
    "cleaned_data = qa_data.remove_columns([\"context\", \"category\"])\n",
    "\n",
    "# Display the final dataset information\n",
    "print(f\"Final dataset contains {len(cleaned_data)} examples.\")\n",
    "print(f\"Fields in final dataset: {list(cleaned_data.features.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d42a05a-806e-4438-9361-8ea12bada1d6",
   "metadata": {},
   "source": [
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Loading the Dataset**:\n",
    "   The dataset `databricks/databricks-dolly-15k` is loaded from the Hugging Face hub using the `load_dataset` function. The script loads the \"train\" split of the dataset into a variable called `dataset`.\n",
    "\n",
    "2. **Displaying Sample Data**:\n",
    "   The script prints the first instruction and its corresponding response from the dataset to give a quick preview of the data.\n",
    "\n",
    "3. **Filtering the Dataset**:\n",
    "   The dataset is filtered to keep only examples that belong to the following categories:\n",
    "   - `\"close_qa\"`\n",
    "   - `\"open_qa\"`\n",
    "   - `\"general_qa\"`\n",
    "\n",
    "   The filter function iterates through the dataset and includes only those examples whose 'category' field matches one of the categories above.\n",
    "\n",
    "4. **Displaying Filtered Data Information**:\n",
    "   The script prints out the number of examples that remain after filtering and displays the first few categories present in the filtered dataset.\n",
    "\n",
    "5. **Removing Unwanted Fields**:\n",
    "   The columns `\"context\"` and `\"category\"` are removed from the dataset to simplify it for further processing. After this step, only the relevant fields, such as `instruction` and `response`, remain.\n",
    "\n",
    "6. **Final Dataset Information**:\n",
    "   The script then prints the final number of examples and the names of the remaining fields in the dataset after the cleanup process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b7f417-88c6-4155-bdd4-041f97a1827e",
   "metadata": {},
   "source": [
    "## Dataset Formatting and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5009c51-a2b8-48c4-b502-3dc7de6ad24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompts(batch):\n",
    "    formatted_prompts = []\n",
    "    for instruction, response in zip(batch[\"instruction\"], batch[\"response\"]):\n",
    "        prompt = f\"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "        formatted_prompts.append(prompt)\n",
    "    return {\"text\": formatted_prompts}\n",
    "\n",
    "# Apply the formatting function to the dataset in a batched manner\n",
    "dataset = cleaned_data.map(format_prompts, batched=True)\n",
    "\n",
    "# Split the dataset into train and validation sets with a 80-20 split\n",
    "train_dataset, validation_dataset = dataset.train_test_split(test_size=0.2, seed=99).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c9c58c-3342-4268-b6b6-622eaff7de10",
   "metadata": {},
   "source": [
    "## Fine-Tuning a Model with SFTTrainer and LoRA\n",
    "\n",
    "This script fine-tunes a model using the `SFTTrainer` from the `trl` library, with the application of the LoRA technique for efficient model adaptation. It sets up the training process, customizes various training arguments, and includes optimizations for memory and speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b46a687-41db-494a-9f63-7f60535210c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning for max number of steps: 1480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 951 examples [00:00, 1515.61 examples/s]\n",
      "Generating train split: 259 examples [00:00, 1459.73 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5948, 'grad_norm': 0.287109375, 'learning_rate': 9.815078236130868e-06, 'epoch': 1.680672268907563}\n",
      "{'eval_loss': 2.4375014305114746, 'eval_runtime': 3.8475, 'eval_samples_per_second': 67.317, 'eval_steps_per_second': 8.577, 'epoch': 1.680672268907563}\n",
      "{'loss': 2.479, 'grad_norm': 0.22265625, 'learning_rate': 9.103840682788051e-06, 'epoch': 3.361344537815126}\n",
      "{'eval_loss': 2.377213478088379, 'eval_runtime': 3.9288, 'eval_samples_per_second': 65.924, 'eval_steps_per_second': 8.4, 'epoch': 3.361344537815126}\n",
      "{'loss': 2.4481, 'grad_norm': 0.240234375, 'learning_rate': 8.392603129445237e-06, 'epoch': 5.042016806722689}\n",
      "{'eval_loss': 2.361212730407715, 'eval_runtime': 3.9084, 'eval_samples_per_second': 66.267, 'eval_steps_per_second': 8.443, 'epoch': 5.042016806722689}\n",
      "{'loss': 2.414, 'grad_norm': 0.291015625, 'learning_rate': 7.681365576102418e-06, 'epoch': 6.722689075630252}\n",
      "{'eval_loss': 2.3490004539489746, 'eval_runtime': 3.9152, 'eval_samples_per_second': 66.152, 'eval_steps_per_second': 8.429, 'epoch': 6.722689075630252}\n",
      "{'loss': 2.4096, 'grad_norm': 0.251953125, 'learning_rate': 6.970128022759603e-06, 'epoch': 8.403361344537815}\n",
      "{'eval_loss': 2.3423309326171875, 'eval_runtime': 3.9041, 'eval_samples_per_second': 66.341, 'eval_steps_per_second': 8.453, 'epoch': 8.403361344537815}\n",
      "{'loss': 2.4036, 'grad_norm': 0.2578125, 'learning_rate': 6.258890469416786e-06, 'epoch': 10.084033613445378}\n",
      "{'eval_loss': 2.3381478786468506, 'eval_runtime': 3.9474, 'eval_samples_per_second': 65.613, 'eval_steps_per_second': 8.36, 'epoch': 10.084033613445378}\n",
      "{'loss': 2.3797, 'grad_norm': 0.291015625, 'learning_rate': 5.547652916073969e-06, 'epoch': 11.764705882352942}\n",
      "{'eval_loss': 2.335531234741211, 'eval_runtime': 3.8926, 'eval_samples_per_second': 66.537, 'eval_steps_per_second': 8.478, 'epoch': 11.764705882352942}\n",
      "{'loss': 2.3798, 'grad_norm': 0.259765625, 'learning_rate': 4.8364153627311525e-06, 'epoch': 13.445378151260504}\n",
      "{'eval_loss': 2.334115505218506, 'eval_runtime': 3.9316, 'eval_samples_per_second': 65.877, 'eval_steps_per_second': 8.394, 'epoch': 13.445378151260504}\n",
      "{'loss': 2.3825, 'grad_norm': 0.263671875, 'learning_rate': 4.125177809388336e-06, 'epoch': 15.126050420168067}\n",
      "{'eval_loss': 2.3325533866882324, 'eval_runtime': 6.4106, 'eval_samples_per_second': 40.402, 'eval_steps_per_second': 5.148, 'epoch': 15.126050420168067}\n",
      "{'loss': 2.3621, 'grad_norm': 0.2890625, 'learning_rate': 3.413940256045519e-06, 'epoch': 16.80672268907563}\n",
      "{'eval_loss': 2.3315019607543945, 'eval_runtime': 3.9047, 'eval_samples_per_second': 66.33, 'eval_steps_per_second': 8.451, 'epoch': 16.80672268907563}\n",
      "{'loss': 2.367, 'grad_norm': 0.31640625, 'learning_rate': 2.702702702702703e-06, 'epoch': 18.48739495798319}\n",
      "{'eval_loss': 2.3309085369110107, 'eval_runtime': 3.8898, 'eval_samples_per_second': 66.585, 'eval_steps_per_second': 8.484, 'epoch': 18.48739495798319}\n",
      "{'loss': 2.3612, 'grad_norm': 0.291015625, 'learning_rate': 1.9914651493598865e-06, 'epoch': 20.168067226890756}\n",
      "{'eval_loss': 2.3304283618927, 'eval_runtime': 3.813, 'eval_samples_per_second': 67.926, 'eval_steps_per_second': 8.655, 'epoch': 20.168067226890756}\n",
      "{'loss': 2.3509, 'grad_norm': 0.287109375, 'learning_rate': 1.2802275960170698e-06, 'epoch': 21.84873949579832}\n",
      "{'eval_loss': 2.3301663398742676, 'eval_runtime': 3.9162, 'eval_samples_per_second': 66.136, 'eval_steps_per_second': 8.427, 'epoch': 21.84873949579832}\n",
      "{'loss': 2.3576, 'grad_norm': 0.310546875, 'learning_rate': 5.689900426742532e-07, 'epoch': 23.529411764705884}\n",
      "{'eval_loss': 2.330021619796753, 'eval_runtime': 3.8621, 'eval_samples_per_second': 67.062, 'eval_steps_per_second': 8.545, 'epoch': 23.529411764705884}\n",
      "{'train_runtime': 3358.8183, 'train_samples_per_second': 7.05, 'train_steps_per_second': 0.441, 'train_loss': 2.4035323374980204, 'epoch': 24.873949579831933}\n",
      "Time:  3358.82\n",
      "Samples/second:  7.05\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import warnings\n",
    "from transformers import logging as transformers_logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "transformers_logging.set_verbosity_error()\n",
    " \n",
    "from trl import SFTTrainer\n",
    " \n",
    "finetuned_model_id = \"qwen-0.5B-qa\"\n",
    "\n",
    "# Calculate max_steps based on the subset size\n",
    "num_train_samples = len(train_dataset)\n",
    "batch_size = 2\n",
    "gradient_accumulation_steps = 8\n",
    "steps_per_epoch = num_train_samples // (batch_size * gradient_accumulation_steps)\n",
    "num_epochs = 5\n",
    "max_steps = steps_per_epoch * num_epochs\n",
    "print(f\"Finetuning for max number of steps: {max_steps}\")\n",
    "\n",
    "def print_training_summary(results):\n",
    "    print(f\"Time: {results.metrics['train_runtime']: .2f}\")\n",
    "    print(f\"Samples/second: {results.metrics['train_samples_per_second']: .2f}\")\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_ratio=0.05,\n",
    "        max_steps=max_steps,\n",
    "        learning_rate=1e-5,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_steps=500,\n",
    "        bf16=True,\n",
    "        logging_steps=100,\n",
    "        output_dir=finetuned_model_id,\n",
    "        use_ipex=True,\n",
    "        max_grad_norm=0.6,\n",
    "        weight_decay=0.01,\n",
    "        group_by_length=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    packing=True\n",
    ")\n",
    "\n",
    "if device != \"cpu\":\n",
    "    torch.xpu.empty_cache()\n",
    "results = trainer.train()\n",
    "print_training_summary(results)\n",
    "\n",
    "# save lora model\n",
    "tuned_lora_model = \"Qwen2.5-0.5B-qa-lora\"\n",
    "trainer.model.save_pretrained(tuned_lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7107b663-b520-4000-b1b9-dfc33779ccd8",
   "metadata": {},
   "source": [
    "### Key Parameters\n",
    "\n",
    "### Training Parameters:\n",
    "   - **`num_train_samples`**: The total number of training samples in the `train_dataset`.\n",
    "   - **`batch_size`**: The batch size for training (set to 2).\n",
    "   - **`gradient_accumulation_steps`**: The number of steps to accumulate gradients before updating the model (set to 8).\n",
    "   - **`steps_per_epoch`**: Calculated based on the number of training samples, batch size, and gradient accumulation steps.\n",
    "   - **`num_epochs`**: The number of epochs (set to 5).\n",
    "   - **`max_steps`**: The total number of training steps is computed by multiplying `steps_per_epoch` with `num_epochs`.\n",
    "   - The script prints out the calculated number of maximum steps for finetuning.\n",
    "\n",
    "### Training Arguments:\n",
    "   The `transformers.TrainingArguments` are set up with the following parameters:\n",
    "   - **`per_device_train_batch_size`**: The batch size used per device (set to 2).\n",
    "   - **`gradient_accumulation_steps`**: The number of gradient accumulation steps (set to 8).\n",
    "   - **`warmup_ratio`**: Ratio of warmup steps (set to 0.05).\n",
    "   - **`max_steps`**: The maximum number of training steps (calculated earlier).\n",
    "   - **`learning_rate`**: The learning rate for training (set to `1e-5`).\n",
    "   - **`evaluation_strategy`**: Defines how often evaluation should happen (set to \"steps\").\n",
    "   - **`save_steps`**: Defines the frequency of saving the model (set to 500 steps).\n",
    "   - **`bf16`**: Specifies usage of bfloat16 precision for better training speed and memory efficiency.\n",
    "   - **`logging_steps`**: Defines the frequency of logging (set to 100 steps).\n",
    "   - **`output_dir`**: Directory where the fine-tuned model will be saved.\n",
    "   - **`use_ipex`**: Enables the Intel Extension for PyTorch to optimize model training on Intel hardware.\n",
    "   - **`max_grad_norm`**: Gradient clipping (set to 0.6).\n",
    "   - **`weight_decay`**: Weight decay for regularization (set to 0.01).\n",
    "   - **`group_by_length`**: Optimizes batching by grouping examples with similar sequence lengths.\n",
    "\n",
    "### Trainer Setup:\n",
    "   The `SFTTrainer` is instantiated with the following parameters:\n",
    "   - **`model`**: The model to be fine-tuned.\n",
    "   - **`train_dataset`**: The training dataset.\n",
    "   - **`eval_dataset`**: The validation dataset.\n",
    "   - **`tokenizer`**: The tokenizer used for encoding the input.\n",
    "   - **`args`**: The training arguments defined earlier.\n",
    "   - **`peft_config`**: The LoRA configuration for efficient fine-tuning.\n",
    "   - **`dataset_text_field`**: The name of the text field in the dataset (set to `\"text\"`).\n",
    "   - **`max_seq_length`**: Maximum sequence length for input examples (set to 512).\n",
    "   - **`packing`**: Whether to pack sequences to optimize training.\n",
    "\n",
    "### Key Techniques:\n",
    "- **LoRA**: Low-Rank Adaptation (LoRA) is used for efficient fine-tuning, enabling parameter-efficient training.\n",
    "- **`SFTTrainer`**: A custom trainer for fine-tuning models with special support for sequence-to-sequence tasks and additional features like LoRA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77564a3d-10c4-4d2c-ae17-523b135207ee",
   "metadata": {},
   "source": [
    "## Let's test our finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a1257fa-3128-427e-9169-a1838798bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "tuned_model = \"qwen-0.5B-qa\"\n",
    "tuned_lora_model = \"Qwen2.5-0.5B-qa-lora\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, tuned_lora_model)\n",
    "model = model.merge_and_unload()\n",
    "# save final tuned model\n",
    "model.save_pretrained(tuned_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82bd24f-5b53-4067-9b18-beafb2164927",
   "metadata": {},
   "source": [
    "## Testing our finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41117602-99b5-45be-a67f-59d4b9dc6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = [\n",
    "    \"Who are the authors of the paper-Attention all you need?\",\n",
    "    \"List out the formulas given the paper.\",\n",
    "    \"What is llama?\",\n",
    "    \"What are the different types of finetuning techniques?\",\n",
    "    \"What is Gen-AI?\",\n",
    "    \"What are the difference between Machine Learning algorithms and AI algorithms?\"\n",
    "]\n",
    "device = \"xpu:0\"\n",
    "\n",
    "model = model.to(device)\n",
    "for text in test_inputs:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200, \n",
    "                             do_sample=False, top_k=100,temperature=0.1, \n",
    "                             eos_token_id=tokenizer.eos_token_id)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    print(\"____\"*25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune_ve",
   "language": "python",
   "name": "finetune_ve"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
